#!/usr/bin/env python3
"""
Workflow: fetch_academic_paper
Description: ç»Ÿä¸€è®ºæ–‡è·å–æ¥å£ï¼Œæ ¹æ®æ¥æºè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ Atomic Recipe
Created: 2025-11-25
Version: 2.0.0
"""

import json
import re
import sys
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed


# å¦‚æœä¸åœ¨ frago ç¯å¢ƒä¸­è¿è¡Œï¼Œéœ€è¦æ‰‹åŠ¨å¯¼å…¥
try:
    from frago.recipes import RecipeRunner, RecipeExecutionError
except ImportError:
    # å›é€€æ–¹æ¡ˆï¼šç›´æ¥è°ƒç”¨è„šæœ¬æ–‡ä»¶
    import subprocess

    class RecipeRunner:
        def run(self, recipe_name: str, params: dict) -> dict:
            """ç®€å•çš„ Recipe æ‰§è¡Œå™¨ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
            # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
            possible_paths = [
                Path(__file__).parent.parent / "atomic" / "system" / f"{recipe_name}.py",
                Path(__file__).parent.parent / "examples" / "atomic" / "system" / f"{recipe_name}.py",
            ]

            script_path = None
            for path in possible_paths:
                if path.exists():
                    script_path = path
                    break

            if not script_path:
                raise FileNotFoundError(f"Recipe not found: {recipe_name}")

            try:
                result = subprocess.run(
                    ["python3", str(script_path), json.dumps(params)],
                    capture_output=True,
                    text=True,
                    timeout=120
                )
                if result.stdout:
                    return {"success": True, "data": json.loads(result.stdout)}
                else:
                    return {"success": False, "error": result.stderr or "No output"}
            except Exception as e:
                return {"success": False, "error": str(e)}

    class RecipeExecutionError(Exception):
        def __init__(self, recipe_name, runtime, exit_code, stderr):
            self.recipe_name = recipe_name
            self.runtime = runtime
            self.exit_code = exit_code
            self.stderr = stderr


def detect_source(identifier: str) -> str:
    """
    æ£€æµ‹è®ºæ–‡æ¥æº

    Args:
        identifier: è®ºæ–‡æ ‡è¯†ç¬¦ï¼ˆURL æˆ– IDï¼‰

    Returns:
        æ¥æºç±»å‹: 'arxiv', 'pubmed', 'pmc', æˆ– 'unknown'
    """
    identifier_lower = identifier.lower()

    # arXiv
    if 'arxiv.org' in identifier_lower:
        return 'arxiv'
    if re.match(r'^\d{4}\.\d{4,5}$', identifier):  # æ–°æ ¼å¼ ID
        return 'arxiv'
    if re.match(r'^[a-z-]+/\d{7}$', identifier_lower):  # æ—§æ ¼å¼ ID
        return 'arxiv'

    # PMC - æ”¯æŒæ–°æ—§ä¸¤ç§ URL æ ¼å¼
    # æ—§æ ¼å¼: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1234567/
    # æ–°æ ¼å¼: https://pmc.ncbi.nlm.nih.gov/articles/PMC1234567/
    if 'pmc' in identifier_lower and 'articles/pmc' in identifier_lower:
        return 'pmc'
    if re.match(r'^PMC\d+$', identifier, re.IGNORECASE):
        return 'pmc'

    # PubMed
    if 'pubmed.ncbi.nlm.nih.gov' in identifier_lower:
        return 'pubmed'
    if re.match(r'^\d{7,8}$', identifier):  # 7-8ä½æ•°å­—å¯èƒ½æ˜¯ PMID
        return 'pubmed'

    return 'unknown'


def fetch_single_paper(identifier: str, output_dir: str, runner: RecipeRunner, delay: float = 0) -> dict:
    """
    è·å–å•ç¯‡è®ºæ–‡

    Args:
        identifier: è®ºæ–‡æ ‡è¯†ç¬¦
        output_dir: è¾“å‡ºç›®å½•
        runner: Recipe æ‰§è¡Œå™¨
        delay: è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰

    Returns:
        è·å–ç»“æœ
    """
    # æ·»åŠ å»¶è¿Ÿï¼ˆæ‰¹é‡ä¸‹è½½æ—¶é¿å…é™æµï¼‰
    if delay > 0:
        time.sleep(delay)

    source = detect_source(identifier)

    if source == 'unknown':
        return {
            'success': False,
            'identifier': identifier,
            'error': f'æ— æ³•è¯†åˆ«è®ºæ–‡æ¥æº: {identifier}'
        }

    # é€‰æ‹©åˆé€‚çš„ Recipe
    if source == 'arxiv':
        recipe_name = 'arxiv_fetch_paper'
        # arXiv ä½¿ç”¨ç›´æ¥ PDF ä¸‹è½½ï¼Œä¸éœ€è¦ formats å‚æ•°
        params = {
            'identifier': identifier,
            'output_dir': output_dir
        }
    else:  # pubmed æˆ– pmc
        recipe_name = 'pubmed_fetch_paper'
        params = {
            'identifier': identifier,
            'output_dir': output_dir,
            'formats': ['pdf', 'xml', 'abstract']
        }

    print(f"ğŸ“¥ æ­£åœ¨è·å–: {identifier} (æ¥æº: {source})", file=sys.stderr)

    try:
        result = runner.run(recipe_name, params=params)

        # å¤„ç† RecipeRunner è¿”å›çš„åµŒå¥—ç»“æ„
        if result.get('success') and result.get('data'):
            inner_data = result['data']
            inner_data['detected_source'] = source
            inner_data['recipe_used'] = recipe_name
            return inner_data
        elif result.get('success') and 'content' in result:
            # ç›´æ¥è¿”å›çš„ç»“æœ
            result['detected_source'] = source
            result['recipe_used'] = recipe_name
            return result
        else:
            return {
                'success': False,
                'identifier': identifier,
                'detected_source': source,
                'error': result.get('error', 'Unknown error')
            }

    except Exception as e:
        return {
            'success': False,
            'identifier': identifier,
            'detected_source': source,
            'error': {
                'type': type(e).__name__,
                'message': str(e)
            }
        }


def main():
    """ä¸»å‡½æ•°ï¼šç»Ÿä¸€è®ºæ–‡è·å–å…¥å£"""

    # è§£æè¾“å…¥å‚æ•°
    if len(sys.argv) < 2:
        params = {}
    else:
        try:
            params = json.loads(sys.argv[1])
        except json.JSONDecodeError as e:
            print(json.dumps({
                "success": False,
                "error": f"å‚æ•° JSON è§£æå¤±è´¥: {e}"
            }), file=sys.stderr)
            sys.exit(1)

    # æ”¯æŒå•ä¸ªæˆ–å¤šä¸ªè®ºæ–‡
    identifiers = params.get('identifiers') or params.get('papers') or []

    # ä¹Ÿæ”¯æŒå•ä¸ª identifier å‚æ•°
    single_id = params.get('identifier') or params.get('url')
    if single_id:
        identifiers = [single_id]

    if not identifiers:
        print(json.dumps({
            "success": False,
            "error": "ç¼ºå°‘å¿…éœ€å‚æ•°: identifiers, papers, identifier æˆ– url"
        }), file=sys.stderr)
        sys.exit(1)

    # å…¶ä»–å‚æ•°
    output_dir = params.get('output_dir', './papers')
    parallel = params.get('parallel', False)  # é»˜è®¤ä¸²è¡Œï¼Œé¿å…è§¦å‘é™æµ
    max_workers = params.get('max_workers', 2)
    delay = params.get('delay', 3.0)  # é»˜è®¤ 3 ç§’å»¶è¿Ÿ

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # åˆå§‹åŒ– Recipe Runner
    runner = RecipeRunner()

    print(f"ğŸ“š å‡†å¤‡è·å– {len(identifiers)} ç¯‡è®ºæ–‡", file=sys.stderr)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}", file=sys.stderr)
    if not parallel:
        print(f"â±ï¸  è¯·æ±‚é—´éš”: {delay}s", file=sys.stderr)

    results = []
    stats = {
        'total': len(identifiers),
        'success': 0,
        'failed': 0,
        'sources': {}
    }

    if parallel and len(identifiers) > 1:
        # å¹¶è¡Œè·å–ï¼ˆæ³¨æ„ï¼šå¯èƒ½è§¦å‘é™æµï¼‰
        print("âš ï¸  å¹¶è¡Œæ¨¡å¼ï¼šå¯èƒ½è§¦å‘ arXiv é™æµ", file=sys.stderr)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_id = {
                executor.submit(fetch_single_paper, id_, output_dir, runner, 0): id_
                for id_ in identifiers
            }

            for future in as_completed(future_to_id):
                paper_id = future_to_id[future]
                try:
                    result = future.result()
                    results.append(result)

                    if result.get('success'):
                        stats['success'] += 1
                        source = result.get('detected_source', 'unknown')
                        stats['sources'][source] = stats['sources'].get(source, 0) + 1
                        file_size = result.get('content', {}).get('file_size_mb', '?')
                        print(f"âœ… æˆåŠŸ: {paper_id} ({file_size} MB)", file=sys.stderr)
                    else:
                        stats['failed'] += 1
                        print(f"âŒ å¤±è´¥: {paper_id}", file=sys.stderr)

                except Exception as e:
                    stats['failed'] += 1
                    results.append({
                        'success': False,
                        'identifier': paper_id,
                        'error': str(e)
                    })
                    print(f"âŒ å¼‚å¸¸: {paper_id} - {e}", file=sys.stderr)
    else:
        # ä¸²è¡Œè·å–ï¼ˆæ¨èï¼Œé¿å…é™æµï¼‰
        for i, id_ in enumerate(identifiers):
            # ç¬¬ä¸€ä¸ªä¸éœ€è¦å»¶è¿Ÿ
            actual_delay = delay if i > 0 else 0
            result = fetch_single_paper(id_, output_dir, runner, actual_delay)
            results.append(result)

            if result.get('success'):
                stats['success'] += 1
                source = result.get('detected_source', 'unknown')
                stats['sources'][source] = stats['sources'].get(source, 0) + 1
                file_size = result.get('content', {}).get('file_size_mb', '?')
                print(f"âœ… æˆåŠŸ: {id_} ({file_size} MB)", file=sys.stderr)
            else:
                stats['failed'] += 1
                error_msg = result.get('error', {})
                if isinstance(error_msg, dict):
                    error_msg = error_msg.get('message', 'Unknown error')
                print(f"âŒ å¤±è´¥: {id_} - {error_msg}", file=sys.stderr)

    # è¾“å‡ºç»“æœ
    output = {
        'success': stats['failed'] == 0,
        'workflow': 'fetch_academic_paper',
        'output_dir': output_dir,
        'stats': stats,
        'results': results
    }

    print(json.dumps(output, ensure_ascii=False, indent=2))

    # è‡³å°‘æœ‰ä¸€ä¸ªæˆåŠŸå°±è¿”å› 0
    sys.exit(0 if stats['success'] > 0 else 1)


if __name__ == "__main__":
    main()
