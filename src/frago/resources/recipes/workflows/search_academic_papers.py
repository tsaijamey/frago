#!/usr/bin/env python3
"""
Workflow: search_academic_papers
Description: å¹¶è¡ŒæŸ¥è¯¢å¤šä¸ªå­¦æœ¯æ•°æ®åº“ï¼ˆarXiv + PubMedï¼‰ï¼Œåˆå¹¶ç»“æœå¹¶æŒ‰æ—¶é—´æ’åº
Created: 2025-11-24
Version: 1.0.0
"""

import json
import sys
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime


# å¦‚æœä¸åœ¨ frago ç¯å¢ƒä¸­è¿è¡Œï¼Œéœ€è¦æ‰‹åŠ¨å¯¼å…¥
try:
    from frago.recipes import RecipeRunner, RecipeExecutionError
except ImportError:
    # å›é€€æ–¹æ¡ˆï¼šç›´æ¥è°ƒç”¨è„šæœ¬æ–‡ä»¶
    import subprocess

    class RecipeRunner:
        def run(self, recipe_name: str, params: dict) -> dict:
            """ç®€å•çš„ Recipe æ‰§è¡Œå™¨ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
            script_path = Path(__file__).parent.parent / "examples" / "atomic" / "system" / f"{recipe_name}.py"
            if not script_path.exists():
                raise FileNotFoundError(f"Recipe not found: {recipe_name}")

            try:
                result = subprocess.run(
                    ["python3", str(script_path), json.dumps(params)],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                return json.loads(result.stdout) if result.stdout else {"success": False, "error": "No output"}
            except Exception as e:
                return {"success": False, "error": str(e)}

    class RecipeExecutionError(Exception):
        def __init__(self, recipe_name, runtime, exit_code, stderr):
            self.recipe_name = recipe_name
            self.runtime = runtime
            self.exit_code = exit_code
            self.stderr = stderr


def normalize_date(date_str: str, source: str) -> str:
    """
    ç»Ÿä¸€æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DD

    Args:
        date_str: åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²
        source: æ•°æ®æºï¼ˆarXiv æˆ– PubMedï¼‰

    Returns:
        æ ‡å‡†åŒ–çš„æ—¥æœŸå­—ç¬¦ä¸²
    """
    try:
        if source == 'arXiv':
            # arXiv æ ¼å¼: YYYY-MM-DD
            return date_str[:10]
        elif source == 'PubMed':
            # PubMed æ ¼å¼: YYYY Mon DD (å¦‚ "2025 Nov 23")
            dt = datetime.strptime(date_str, "%Y %b %d")
            return dt.strftime("%Y-%m-%d")
        else:
            return date_str
    except:
        return date_str


def search_database(database: str, query: str, max_results: int, runner: RecipeRunner) -> dict:
    """
    åœ¨å•ä¸ªæ•°æ®åº“ä¸­æœç´¢

    Args:
        database: æ•°æ®åº“åç§°ï¼ˆarxiv æˆ– pubmedï¼‰
        query: æœç´¢å…³é”®è¯
        max_results: æœ€å¤§è¿”å›ç»“æœæ•°
        runner: Recipe æ‰§è¡Œå™¨

    Returns:
        æœç´¢ç»“æœå­—å…¸
    """
    recipe_map = {
        'arxiv': 'arxiv_search_papers',
        'pubmed': 'pubmed_search_papers'
    }

    recipe_name = recipe_map.get(database.lower())
    if not recipe_name:
        return {
            'success': False,
            'source': database,
            'error': f'ä¸æ”¯æŒçš„æ•°æ®åº“: {database}'
        }

    try:
        result = runner.run(recipe_name, params={
            'query': query,
            'max_results': max_results
        })

        # å¤„ç† RecipeRunner è¿”å›çš„åµŒå¥—ç»“æ„
        # result ç»“æ„: {"success": bool, "data": {...}, "error": ...}
        if result.get('success') and result.get('data'):
            # è¿”å›å†…éƒ¨çš„ data
            inner_data = result['data']
            if not inner_data.get('success'):
                print(f"âš ï¸  {database} æŸ¥è¯¢å¤±è´¥: {inner_data.get('error')}", file=sys.stderr)
            return inner_data
        else:
            print(f"âš ï¸  {database} æŸ¥è¯¢å¤±è´¥: {result.get('error')}", file=sys.stderr)
            return {
                'success': False,
                'source': database,
                'error': result.get('error', 'Unknown error')
            }

    except Exception as e:
        return {
            'success': False,
            'source': database,
            'error': {
                'type': type(e).__name__,
                'message': str(e)
            }
        }


def main():
    """ä¸»å‡½æ•°ï¼šç¼–æ’å¹¶è¡ŒæŸ¥è¯¢å¤šä¸ªæ•°æ®åº“"""

    # è§£æè¾“å…¥å‚æ•°
    if len(sys.argv) < 2:
        params = {}
    else:
        try:
            params = json.loads(sys.argv[1])
        except json.JSONDecodeError as e:
            print(json.dumps({
                "success": False,
                "error": f"å‚æ•° JSON è§£æå¤±è´¥: {e}"
            }), file=sys.stderr)
            sys.exit(1)

    # éªŒè¯å¿…éœ€å‚æ•°
    query = params.get('query') or params.get('keywords')
    if not query:
        print(json.dumps({
            "success": False,
            "error": "ç¼ºå°‘å¿…éœ€å‚æ•°: query æˆ– keywords"
        }), file=sys.stderr)
        sys.exit(1)

    # å¯é€‰å‚æ•°
    databases = params.get('databases', ['arxiv', 'pubmed'])
    max_results_per_db = params.get('max_results', 10)
    sort_by = params.get('sort_by', 'date')  # date æˆ– relevance

    # åˆå§‹åŒ– Recipe Runner
    runner = RecipeRunner()

    # å¹¶è¡ŒæŸ¥è¯¢å¤šä¸ªæ•°æ®åº“
    print(f"ğŸ” å¼€å§‹åœ¨ {len(databases)} ä¸ªæ•°æ®åº“ä¸­æœç´¢: {', '.join(databases)}", file=sys.stderr)

    all_papers = []
    database_stats = {}

    with ThreadPoolExecutor(max_workers=len(databases)) as executor:
        # æäº¤æ‰€æœ‰æŸ¥è¯¢ä»»åŠ¡
        future_to_db = {
            executor.submit(search_database, db, query, max_results_per_db, runner): db
            for db in databases
        }

        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_db):
            db_name = future_to_db[future]
            try:
                result = future.result()

                if result.get('success'):
                    papers = result.get('papers', [])
                    all_papers.extend(papers)
                    database_stats[db_name] = {
                        'success': True,
                        'count': len(papers)
                    }
                    print(f"âœ… {db_name}: {len(papers)} ç¯‡è®ºæ–‡", file=sys.stderr)
                else:
                    database_stats[db_name] = {
                        'success': False,
                        'error': result.get('error')
                    }
                    print(f"âŒ {db_name}: æŸ¥è¯¢å¤±è´¥", file=sys.stderr)

            except Exception as e:
                database_stats[db_name] = {
                    'success': False,
                    'error': str(e)
                }
                print(f"âŒ {db_name}: æ‰§è¡Œå¼‚å¸¸ - {e}", file=sys.stderr)

    # ç»Ÿä¸€æ—¥æœŸæ ¼å¼
    for paper in all_papers:
        paper['published_normalized'] = normalize_date(
            paper.get('published', ''),
            paper.get('source', '')
        )

    # æ’åºç»“æœ
    if sort_by == 'date':
        all_papers.sort(key=lambda p: p.get('published_normalized', ''), reverse=True)
    # relevance æ’åºä¿æŒåŸå§‹é¡ºåºï¼ˆAPI å·²æŒ‰ç›¸å…³æ€§æ’åºï¼‰

    # è¿”å›æ±‡æ€»ç»“æœ
    output = {
        'success': True,
        'workflow': 'search_academic_papers',
        'query': query,
        'databases_queried': databases,
        'total_papers': len(all_papers),
        'database_stats': database_stats,
        'papers': all_papers
    }

    print(json.dumps(output, ensure_ascii=False, indent=2))
    sys.exit(0)


if __name__ == "__main__":
    main()
